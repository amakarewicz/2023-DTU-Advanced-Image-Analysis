# # TODO: EarlyStopping
# min_val_loss = np.inf
# epochs_no_improvement = 0
# patience = 5
# if valid_loss_all[epoch] < min_val_loss:
#     epochs_no_improvement = 0
#     min_val_loss = valid_loss_all[epoch]
# # TODO: add saving a checkpoint (so the net before overfitting returned)
# else: epochs_no_improvement += 1
# if epochs_no_improvement == patience:
#     print('Early stopping')
#     break

# %% TODO
# loss and accuracy loss (maybe add F1score/precision/recall) DONE
# prediction on test set (+some examples to show) DONE
# without augmentation + two different ones (DL report) DONE
# different numbers of channels (with Early stopping to see possible overfitting) [16,32,64] DONE
# different learning rates DONE
# different batch sizes? DONE

# https://pythonguides.com/pytorch-early-stopping/
# https://machinelearningmastery.com/managing-a-pytorch-training-process-with-checkpoints-and-early-stopping/